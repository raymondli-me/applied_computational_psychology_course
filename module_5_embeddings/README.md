# Module 5: Text Embeddings

**The Geometry of Meaning**

## Learning Objectives

By the end of this module, you will be able to:
- Understand what embeddings are and why they matter
- Load and work with pre-computed embeddings
- Calculate cosine similarity between texts
- Interpret similarity scores

## Key Dataset

**Clara's BERT Embeddings** - 12,915 political texts with 768-dimensional embeddings

```python
df = load_dataset("clara_bert_embeddings")
# Columns: Document_Name, Original_Text, Dim_1, Dim_2, ..., Dim_768
```

## Materials

| File | Description |
|------|-------------|
| [lecture_notes.md](lecture_notes.md) | Lecture content (~37 min) |
| [lab_instructions.md](lab_instructions.md) | Hands-on exercises (60 min) |
| [assessment.md](assessment.md) | Test your understanding |
| [answer_key.md](answer_key.md) | Model answers |
| [flashcards.md](flashcards.md) | 22 study cards |

## Estimated Time

~2 hours total
