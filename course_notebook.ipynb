{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Psychological Data Science\n",
    "## Course Notebook - Your Workspace for the Semester\n",
    "\n",
    "**Welcome!** This notebook is your workspace for the entire course.\n",
    "\n",
    "### Quick Start\n",
    "1. **Save a copy**: File → Save a copy in Drive\n",
    "2. **Run the setup cell below** (just once)\n",
    "3. **Follow your lab instructions** and paste code here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run This Setup Cell (Once Per Session)\n",
    "\n",
    "This loads the course dataset system. Run this cell every time you open the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COURSE SETUP - Run this cell first!\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "# Course dataset configuration\n",
    "GCS_BUCKET = \"variable-resolution-applied-computational-psychology-course\"\n",
    "GCS_BASE_URL = f\"https://storage.googleapis.com/{GCS_BUCKET}\"\n",
    "CATALOG_URL = f\"{GCS_BASE_URL}/manifest.json\"\n",
    "\n",
    "# Load the dataset catalog\n",
    "print(\"Loading course dataset catalog...\")\n",
    "with urllib.request.urlopen(CATALOG_URL) as response:\n",
    "    CATALOG = json.loads(response.read().decode())\n",
    "\n",
    "print(f\"✓ Connected! {len(CATALOG['datasets'])} datasets available\")\n",
    "\n",
    "# ============================================================\n",
    "# MAIN FUNCTION: load_dataset()\n",
    "# ============================================================\n",
    "\n",
    "def load_dataset(name, nrows=None):\n",
    "    \"\"\"\n",
    "    Load a course dataset by name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The dataset name (e.g., \"andrea_reddit_results_andrea_2025_03_13\")\n",
    "    nrows : int, optional\n",
    "        Number of rows to load (useful for large datasets)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> df = load_dataset(\"andrea_reddit_results_andrea_2025_03_13\", nrows=5000)\n",
    "    >>> df.head()\n",
    "    \"\"\"\n",
    "    for ds in CATALOG['datasets']:\n",
    "        if ds['canonical_name'] == name:\n",
    "            url = ds['access']['public_url']\n",
    "            df = pd.read_csv(url, nrows=nrows)\n",
    "            print(f\"✓ Loaded {len(df):,} rows from '{name}'\")\n",
    "            return df\n",
    "    \n",
    "    # Dataset not found - show similar names\n",
    "    similar = [ds['canonical_name'] for ds in CATALOG['datasets'] \n",
    "               if name.lower() in ds['canonical_name'].lower()][:5]\n",
    "    raise ValueError(f\"Dataset '{name}' not found. Similar: {similar}\")\n",
    "\n",
    "\n",
    "def list_datasets(contributor=None, search=None):\n",
    "    \"\"\"\n",
    "    List available datasets, optionally filtered.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    contributor : str, optional\n",
    "        Filter by contributor name (e.g., \"Andrea\", \"Peter\")\n",
    "    search : str, optional\n",
    "        Search in dataset names\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> list_datasets(contributor=\"Andrea\")\n",
    "    >>> list_datasets(search=\"reddit\")\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for ds in CATALOG['datasets']:\n",
    "        name = ds['canonical_name']\n",
    "        if contributor and contributor.lower() not in name.lower():\n",
    "            continue\n",
    "        if search and search.lower() not in name.lower():\n",
    "            continue\n",
    "        rows = ds.get('stats', {}).get('rows', 'unknown')\n",
    "        results.append(f\"{name} ({rows:,} rows)\" if isinstance(rows, int) else f\"{name}\")\n",
    "    \n",
    "    print(f\"Found {len(results)} datasets:\")\n",
    "    for r in results[:20]:\n",
    "        print(f\"  - {r}\")\n",
    "    if len(results) > 20:\n",
    "        print(f\"  ... and {len(results) - 20} more\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# READY!\n",
    "# ============================================================\n",
    "print(\"\")\n",
    "print(\"Ready! You can now use:\")\n",
    "print(\"  - load_dataset('dataset_name')     → Load a dataset\")\n",
    "print(\"  - load_dataset('name', nrows=1000) → Load first 1000 rows\")\n",
    "print(\"  - list_datasets()                  → See all datasets\")\n",
    "print(\"  - list_datasets(contributor='Andrea') → Filter by contributor\")\n",
    "print(\"\")\n",
    "print(\"Recommended datasets for each module:\")\n",
    "print(\"  M1: andrea_reddit_results_andrea_2025_03_13\")\n",
    "print(\"  M2: agatha_ballet_dancemoms_agatha\")\n",
    "print(\"  M3: yashita_yashita_data, kaitlyn_merged_data_overview_kaitlyn_master\")\n",
    "print(\"  M5: clara_bert_embeddings\")\n",
    "print(\"  M6: raymond_umap_dbscan_results_20250223_154628\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Test It Works\n",
    "\n",
    "Run this cell to make sure everything is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Load a small sample from Andrea's Reddit data\n",
    "df_test = load_dataset(\"andrea_reddit_results_andrea_2025_03_13\", nrows=5)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Workspace\n",
    "\n",
    "Add new cells below as you work through each module's lab instructions.\n",
    "\n",
    "### Module 1: Data Foundations\n",
    "Follow the lab instructions and add your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 1 - Your code here\n",
    "# Follow lab_instructions.md in the module_1_data_foundations folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2 - Your code here\n",
    "# Follow lab_instructions.md in the module_2_linear_regression folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 3: LLMs as Raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 3 - Your code here\n",
    "# You'll need your own OpenAI API key for this module\n",
    "# API_KEY = \"your-api-key-here\"  # Replace with your key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 4: APIs & Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 4 - Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 5: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 5 - Your code here\n",
    "# Load Clara's pre-computed embeddings:\n",
    "# df_embed = load_dataset(\"clara_bert_embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 6: PCA & UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 6 - Your code here\n",
    "# Load Raymond's pre-computed UMAP:\n",
    "# df_umap = load_dataset(\"raymond_umap_dbscan_results_20250223_154628\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
